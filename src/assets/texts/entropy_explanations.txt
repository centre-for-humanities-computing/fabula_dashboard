#### **Word Entropy & Bigram Entropy**: Both word and bigram entropy was calculated by means of Mark Algee-Hewitt’s github for the Stanford Literary Lab pamphlet 17. The code was modified to also asses entropy on the word basis (while pamphlet 17 inly includes bigram-basis). Adopted from Algee-Hewitt's repository. Stopwords were not removed. Measures the “predictability”/amount of information of words or bigrams in the text.
#### **Approximate Entropy**: Approximate Entropy of sentiment arcs calculated per 2 sentences. Sentiment arcs were exctracted with the Vader-lexicon. Approximate entropy is a technique used to quantify the amount of regularity and the unpredictability of fluctuations over time-series data. We compute ApEn with Neurokit2 Settings : app_ent = nk.entropy_approximate(sentarc, dimension=2, tolerance='sd')